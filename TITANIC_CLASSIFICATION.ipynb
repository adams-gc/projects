{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMrfIC9e7xMj6hU3C9bBvse",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adams-gc/projects/blob/main/TITANIC_CLASSIFICATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set plot style\n",
        "plt.style.use('ggplot')\n"
      ],
      "metadata": {
        "id": "61vZlSlUr2Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Titanic dataset\n",
        "train_data = pd.read_csv('/content/train (1).csv')\n",
        "test_data = pd.read_csv('/content/test (1).csv')\n"
      ],
      "metadata": {
        "id": "AzbTy73lsCgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick data overview\n",
        "print(f\"############Train data shape:#####################\")\n",
        "print(f\"Train data shape: {train_data.shape}\")\n",
        "print(f\"###############Test data shape:############## \")\n",
        "print(f\"Test data shape: {test_data.shape}\")\n",
        "print(f\"###############Train data head:############## \")\n",
        "print(train_data.info())\n",
        "print(f\"###############Train data head:############## \")\n",
        "print(train_data.describe())\n",
        "print(f\"###############Train data null:############## \")\n",
        "print(f\"Null values:\\n{train_data.isnull().sum()}\")\n"
      ],
      "metadata": {
        "id": "AxWm4PAYsHGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing missing values\n",
        "sns.heatmap(train_data.isnull(), cbar=False)\n",
        "plt.title(\"Missing Values Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-bvyFmxMsXsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics for numerical columns\n",
        "print(\"Summary statistics of numerical columns:\")\n",
        "print(train_data.describe())\n"
      ],
      "metadata": {
        "id": "K0uVHUJfttkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze survival distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "train_data['Survived'].value_counts().plot.pie(autopct='%1.1f%%', ax=axes[0], explode=[0, 0.1], shadow=True)\n",
        "sns.countplot(x='Survived', data=train_data, ax=axes[1])\n",
        "axes[0].set_title('Survival Distribution')\n",
        "axes[1].set_title('Survival Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dmewIpdotd31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gender-based survival analysis\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=train_data, x='Survived', hue='Sex', palette='coolwarm')\n",
        "plt.title(\"Survival Distribution by Gender\")\n",
        "plt.xlabel(\"Survived\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.legend(title='Gender')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lJ6uFcGdtidu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.groupby(['Sex', 'Survived'])['Survived'].count()"
      ],
      "metadata": {
        "id": "FoP5tNfcwBAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see there was a high percentage of Female Survior"
      ],
      "metadata": {
        "id": "FrFaFlk5w88f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class-based survival analysis\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=train_data, x='Survived', hue='Pclass', palette='Set1')\n",
        "plt.title(\"Survival Distribution by Passenger Class\")\n",
        "plt.xlabel(\"Survived\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.legend(title='Passenger Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Et0jsoHDwbXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crosstab of survival and class for a tabular view\n",
        "print(\"Survival counts by class:\")\n",
        "print(pd.crosstab(train_data['Survived'], train_data['Pclass']))\n"
      ],
      "metadata": {
        "id": "00epTiOIxmpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Age distribution analysis by survival\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.violinplot(data=train_data, x='Survived', y='Age', hue='Pclass', split=True, palette='Set3')\n",
        "plt.title(\"Age Distribution by Survival and Class\")\n",
        "plt.xlabel(\"Survived\")\n",
        "plt.ylabel(\"Age\")\n",
        "plt.legend(title='Passenger Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CDRKEB_lx5jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The youngest Passenger: {train_data[\"Age\"].min()}')\n",
        "print(f'The Oldest Passenger: {train_data[\"Age\"].max()}')\n",
        "print(f'The average age Passenger: {round(train_data[\"Age\"].mean(),2)} ')"
      ],
      "metadata": {
        "id": "LlSvfATMyvsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations: class 1 is more survial\n",
        "- The survial Rate from age 20-40 in Pclass 1 is highe\n",
        "\n"
      ],
      "metadata": {
        "id": "XwknWh0MzdlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extract titles from names for better understanding of passenger types\n",
        "# The titles like 'Mr.', 'Mrs.', etc., may help in analyzing age or survival trends\n",
        "train_data['Title'] =train_data['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\n",
        "print(\"Extracted Titles:\")\n",
        "print(train_data['Title'].value_counts())\n"
      ],
      "metadata": {
        "id": "K8trin1Zyyz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing ages based on the average age for each title\n",
        "# Create a dictionary mapping titles to their mean ages\n",
        "title_age_map = train_data.groupby('Title')['Age'].mean().to_dict()\n",
        "train_data['Age'] = train_data.apply(lambda row: title_age_map[row['Title']] if pd.isnull(row['Age']) else row['Age'], axis=1)\n",
        "print(\"Filled missing Age values based on Title averages.\")\n"
      ],
      "metadata": {
        "id": "0sQNq_mb0GpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['Initial']=0\n",
        "for i in train_data:\n",
        "    train_data['Initial'] = train_data['Name'].str.extract(\"([A-Za-z]+)\\.\")\n",
        "pd.crosstab(train_data['Initial'], train_data['Sex']).T"
      ],
      "metadata": {
        "id": "j2FBeOog07h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=train_data"
      ],
      "metadata": {
        "id": "U12shbHQ2iFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Verify if there are any remaining missing values in the 'Age' column\n",
        "print(\"Missing values in 'Age' after imputation:\")\n",
        "print(df['Age'].isnull().sum())"
      ],
      "metadata": {
        "id": "KhJEbdDh1I75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the age distribution after filling missing values\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(df['Age'], bins=20, kde=True, color='blue')\n",
        "plt.title(\"Age Distribution After Imputation\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TcPWM7Jx2gEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Age'].isnull().any()"
      ],
      "metadata": {
        "id": "pVOrSMZn3G-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embarked"
      ],
      "metadata": {
        "id": "GAqaU2od568-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab([df['Embarked'], df['Pclass']], [df['Sex'],df['Survived']], margins=True).T"
      ],
      "metadata": {
        "id": "GzbLgIqe5MLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values in the 'Embarked' column by filling with the most common value\n",
        "most_common_embarked = df['Embarked'].mode()[0]  # Find the most frequent value\n",
        "df['Embarked'].fillna(most_common_embarked, inplace=True)  # Fill missing values\n",
        "print(f\"Missing values in 'Embarked' after imputation: {df['Embarked'].isnull().sum()}\")\n"
      ],
      "metadata": {
        "id": "josrMfoI32FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the distribution of the 'Embarked' column\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=df, x='Embarked', palette='pastel')\n",
        "plt.title(\"Distribution of Embarked Locations\")\n",
        "plt.xlabel(\"Embarked\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "grVO9RLp6EaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values in 'Fare' column with the median value\n",
        "median_fare = df['Fare'].median()\n",
        "df['Fare'].fillna(median_fare, inplace=True)\n",
        "print(f\"Missing values in 'Fare' after imputation: {df['Fare'].isnull().sum()}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_C4vg2vp4e0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the distribution of the 'Fare' column\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(df['Fare'], bins=30, kde=True, color='green')\n",
        "plt.title(\"Fare Distribution\")\n",
        "plt.xlabel(\"Fare\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6_yLefdN4-a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "majority of the passenger from either class embarked from S"
      ],
      "metadata": {
        "id": "mwYzjQz46Zup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop unnecessary columns that are not useful for analysis or modeling\n",
        "columns_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n",
        "df.drop(columns=columns_to_drop, inplace=True)\n",
        "print(\"Dropped unnecessary columns.\")\n",
        "\n",
        "# Check the updated shape of the dataset after cleaning\n",
        "print(f\"Updated Dataset Shape: {df.shape}\")\n"
      ],
      "metadata": {
        "id": "HCl9cyqg4-wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "feature engineering\n",
        "\n"
      ],
      "metadata": {
        "id": "SF-co4Ds7B3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering: Creating new features to improve model performance\n",
        "\n",
        "# 1. Extracting Family Size from 'SibSp' (siblings/spouses aboard) and 'Parch' (parents/children aboard)\n",
        "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1  # Adding 1 to include the passenger themselves\n",
        "print(\"Created 'FamilySize' feature.\")\n",
        "print(df['FamilySize'])\n"
      ],
      "metadata": {
        "id": "trFa8WcU4-z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Family Size\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=df, x='FamilySize', palette='coolwarm')\n",
        "plt.title(\"Family Size Distribution\")\n",
        "plt.xlabel(\"Family Size\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H8OEDj0j4-3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Creating a binary feature to indicate whether the passenger is traveling alone\n",
        "# 2. Creating a binary feature to indicate whether the passenger is traveling alone\n",
        "df['IsAlone'] = (df['FamilySize'] == 1).astype(int)  # 1 if alone, 0 otherwise\n",
        "print(\"Created 'IsAlone' feature.\")\n",
        "print(df['IsAlone'])"
      ],
      "metadata": {
        "id": "t4KWpgU37mk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Categorizing passengers into Age Groups\n",
        "bins = [0, 12, 18, 35, 60, np.inf]  # Age groups: Child, Teen, Young Adult, Adult, Senior\n",
        "labels = ['Child', 'Teen', 'Young Adult', 'Adult', 'Senior']\n",
        "df['AgeGroup'] = pd.cut(df['Age'], bins=bins, labels=labels)\n",
        "print(\"Created 'AgeGroup' feature.\")\n",
        "print(df['AgeGroup'])\n"
      ],
      "metadata": {
        "id": "4ZkSyYGq7td-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Age Group Distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=df, x='AgeGroup', palette='viridis')\n",
        "plt.title(\"Age Group Distribution\")\n",
        "plt.xlabel(\"Age Group\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zMZrbhZq74aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Creating Fare Per Person Feature\n",
        "# 4. Creating Fare Per Person Feature\n",
        "df['FarePerPerson'] = df['Fare'] / df['FamilySize']  # Average fare per family member\n",
        "print(\"Created 'FarePerPerson' feature.\")\n",
        "print(df['FarePerPerson'])"
      ],
      "metadata": {
        "id": "iktsuVKu8CrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Flagging passengers in premium classes (First Class and high fares)\n",
        "# 5. Flagging passengers in premium classes (First Class and high fares)\n",
        "df['PremiumClass'] = ((df['Pclass'] == 1) & (df['Fare'] > df['Fare'].median())).astype(int)\n",
        "print(\"Created 'PremiumClass' feature.\")\n",
        "print(f'{df[\"PremiumClass\"]}')"
      ],
      "metadata": {
        "id": "tyGZMuNU77kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Premium Class Distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=df, x='PremiumClass', palette='mako')\n",
        "plt.title(\"Premium Class Distribution\")\n",
        "plt.xlabel(\"Premium Class (1: Premium, 0: Not Premium)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-hxFfHW-8Q43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 6. Encoding 'AgeGroup' as numerical for modeling\n",
        "df = pd.get_dummies(df, columns=['AgeGroup'], drop_first=True)\n",
        "print(\"Encoded 'AgeGroup' feature.\")\n",
        "df"
      ],
      "metadata": {
        "id": "ceI0rXEk-oe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Creating Interaction Feature: Survival by Family Size\n",
        "df['SurvivalByFamily'] = df.groupby('FamilySize')['Survived'].transform('mean')\n",
        "print(\"Created 'SurvivalByFamily' feature (mean survival rate by family size).\")\n",
        "df['SurvivalByFamily']"
      ],
      "metadata": {
        "id": "nvVaf3K1-ock"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# 8. Title Normalization (Revisited)\n",
        "# Grouping similar titles into broader categories\n",
        "df['Title'] = df['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major',\n",
        "                                    'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
        "df['Title'] = df['Title'].replace('Mlle', 'Miss')\n",
        "df['Title'] = df['Title'].replace('Ms', 'Miss')\n",
        "df['Title'] = df['Title'].replace('Mme', 'Mrs')\n",
        "print(\"Normalized 'Title' feature.\")\n",
        "\n",
        "# Convert categorical columns ('Sex', 'Embarked', and 'Title') into dummy/indicator variables\n",
        "df = pd.get_dummies(df, columns=['Sex', 'Embarked', 'Title'], drop_first=True)\n",
        "print(\"Converted categorical columns into dummy variables.\")\n",
        "\n",
        "# Preview of the dataset after feature engineering\n",
        "print(\"Dataset after feature engineering:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "HU103R8H-oaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicitive modeling"
      ],
      "metadata": {
        "id": "wPNe2miPBavC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Required Libraries\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "yqeoWz7c-oX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "I546-kBwC-1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=test_data.columns\n",
        "data"
      ],
      "metadata": {
        "id": "SVrP6-gCP-yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Prepare Data for Modeling\n",
        "#Separating features (X) and target variable (y)\n",
        "# X = df.drop(columns=['Survived'])  # Drop unused columns\n",
        "# y = df['Survived']\n",
        "#feature=test_data\n",
        "feature = ['PassengerId','Name', 'Age','Ticket','Fare', 'Cabin']\n",
        "X = test_data.drop(columns=feature) # Changed data to test_data and removed axis argument\n",
        "y = df['Survived']\n",
        "y.dropna()"
      ],
      "metadata": {
        "id": "Kk7jqXvL-oUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode categorical features\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "X"
      ],
      "metadata": {
        "id": "eCXjhh2o-oLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  #Splitting data into training and testing sets (80% train, 20% test)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "#Step 2: Prepare Data for Modeling\n",
        "#Separating features (X) and target variable (y)\n",
        "X = df.drop(columns=['Survived'])  # Use df (train_data) to create X\n",
        "y = df['Survived']\n",
        "y.dropna() # This line doesn't modify y in place; consider y = y.dropna() if needed\n",
        "\n",
        "# One-hot encode categorical features\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Splitting data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "g_goK1Fh-oQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Logistic Regression Model\n",
        "log_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_model.fit(X_train, y_train)\n",
        "y_pred_log = log_model.predict(X_test)\n",
        "\n",
        "# Evaluate Logistic Regression\n",
        "print(\"Logistic Regression Performance:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_log))\n",
        "print(classification_report(y_test, y_pred_log))\n"
      ],
      "metadata": {
        "id": "sMtX9peYFkOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Random Forest Model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate Random Forest\n",
        "print(\"Random Forest Performance:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(classification_report(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "KAM2o9b7-oIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 5: XGBoost Model\n",
        "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate XGBoost\n",
        "print(\"XGBoost Performance:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "print(classification_report(y_test, y_pred_xgb))\n"
      ],
      "metadata": {
        "id": "L4tlmq1c-oFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: ROC Curve and AUC\n",
        "models = {'Logistic Regression': log_model, 'Random Forest': rf_model, 'XGBoost': xgb_model}\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "for name, model in models.items():\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]  # Predicted probabilities for class 1\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    auc_score = roc_auc_score(y_test, y_proba)\n",
        "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)  # Random guess line\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EfNQmj3i-oCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Confusion Matrix for Best Model (Random Forest Example)\n",
        "cm = confusion_matrix(y_test, y_pred_rf)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'])\n",
        "plt.title(\"Confusion Matrix - Random Forest\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i_ql6TTm-n_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade xgboost\n",
        "# !pip install --upgrade scikit-learn\n",
        "# #!pip install scikit-learn==1.0\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jXFCMqVB-n88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "option is to wrap XGBClassifier inside a custom scikit-learn estimator to avoid the __sklearn_tags__ issue. This wrapper will implement a simple interface for XGBClassifier so that scikit-learn's cross-validation can be used."
      ],
      "metadata": {
        "id": "Ep7WY8E1JhHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "# from xgboost import XGBClassifier\n",
        "\n",
        "# class XGBClassifierWrapper(BaseEstimator, ClassifierMixin):\n",
        "#     def __init__(self, **kwargs):\n",
        "#         self.model = XGBClassifier(**kwargs)\n",
        "\n",
        "#     def fit(self, X, y):\n",
        "#         self.model.fit(X, y)\n",
        "#         return self\n",
        "\n",
        "#     def predict(self, X):\n",
        "#         return self.model.predict(X)\n",
        "\n",
        "#     def score(self, X, y):\n",
        "#         return self.model.score(X, y)\n",
        "\n",
        "# # Now use this wrapper in your models dictionary\n",
        "# models = {\n",
        "#     'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "#     'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "#     'XGBoost': XGBClassifierWrapper(n_estimators=100, learning_rate=0.1, random_state=42, use_label_encoder=False, eval_metric='logloss', objective='binary:logistic')\n",
        "# }\n",
        "\n",
        "# # Cross-validation with StratifiedKFold\n",
        "# for name, model in models.items():\n",
        "#     cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "#     cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
        "#     print(f\"{name} Cross-Validation Accuracy Scores: {cv_scores}\")\n",
        "#     print(f\"{name} Mean Cross-Validation Accuracy: {np.mean(cv_scores):.4f}\")\n",
        "#     print(f\"{name} Standard Deviation of Cross-Validation Accuracy: {np.std(cv_scores):.4f}\")\n"
      ],
      "metadata": {
        "id": "Q9kJNtgcJgJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Use XGBClassifier Without cross_val_score\n",
        "If none of the above solutions work, you can perform cross-validation manually by splitting the dataset and fitting the XGBClassifier without using"
      ],
      "metadata": {
        "id": "3c7GHEp1JtWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define the models to compare\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "# Initialize the cross-validation strategy\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Store cross-validation scores for each model\n",
        "cv_scores_dict = {}\n",
        "\n",
        "# Perform cross-validation for each model\n",
        "for name, model in models.items():\n",
        "    cv_scores = []\n",
        "    for train_index, test_index in cv.split(X, y):\n",
        "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        score = model.score(X_test, y_test)\n",
        "        cv_scores.append(score)\n",
        "\n",
        "    cv_scores_dict[name] = np.array(cv_scores)\n",
        "\n",
        "    print(f\"{name} Cross-Validation Accuracy Scores: {cv_scores}\")\n",
        "    print(f\"{name} Mean Cross-Validation Accuracy: {np.mean(cv_scores):.4f}\")\n",
        "    print(f\"{name} Standard Deviation of Cross-Validation Accuracy: {np.std(cv_scores):.4f}\")\n",
        "\n",
        "# Create a box plot for the models' cross-validation accuracy scores\n",
        "plt.figure(figsize=(8, 6))  # Set figure size for better visibility\n",
        "\n",
        "# Draw the boxplot\n",
        "sns.boxplot(data=list(cv_scores_dict.values()), orient='h', palette='Set2')\n",
        "\n",
        "# Set the x-axis labels to the model names\n",
        "plt.yticks(np.arange(len(models)), models.keys(), fontsize=12)\n",
        "\n",
        "# Title and labels\n",
        "plt.title('Model Comparison: Cross-Validation Accuracy Distribution', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Accuracy', fontsize=14)\n",
        "\n",
        "# Show gridlines\n",
        "plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CselBQTiNtmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix\n",
        "Confusion Matrix gives the number of correct and incorrect classifications made by the classifier."
      ],
      "metadata": {
        "id": "KHmd6gxXOKix"
      }
    }
  ]
}